<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Document</title>

  <script defer src="face-api.min.js"></script>
  <script defer src="script.js"></script>

  <style type="text/css">
    * {
      margin: 0;
      padding: 0;
    }
    #canvas, #inputVideo {
      position: absolute;
      top: 0;
      left: 0;
    }

    device  {
      position: absolute;
      top: 700px; left: 300px;
    }
    .d-none {
      display: none;
    }
  </style>
</head>
<body>
  <div></div>
  <div id="webcamBox" style="position: relative" class="margin">
    <video id="inputVideo" width="480" height="320" autoplay></video>
    <canvas id="canvas" width="480" height="320" ></canvas>

    <div class="alert d-none"></div>
  </div>
<!-- <script>
  const video = document.getElementById('inputVideo');
  const canvas = document.getElementById('canvas');

  // const avatar = document.getElementById('avatar')
  const imageUpload = document.getElementById('imageUpload')
  const wrapperBox = document.getElementById('webcamBox')

  function capture(){
    let img = document.querySelector('#screenshot');
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    canvas.getContext('2d').drawImage(video, 0, 0);
    // Other browsers will fall back to image/png
    // img.src = canvas.toDataURL('image/webp');
    // alert("Asds")
  }

  function playVideo(){

      Promise.all([
        faceapi.nets.tinyFaceDetector.loadFromUri('/models'),
        faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
        faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
        faceapi.nets.ssdMobilenetv1.loadFromUri('/models')
        // faceapi.nets.faceExpressionNet.loadFromUri('/models')
      ]).then(startVideo)
      // ]).then(start)


        video.addEventListener('play', () => {
          // const canvas = faceapi.createCanvasFromMedia(video)
          // wrapperBox.append(canvas)
          const displaySize = { width: video.width, height: video.height }
          faceapi.matchDimensions(canvas, displaySize)
          console.log("asdas")
          setInterval(async () => {
            // let image = await faceapi.fetchImage(avatar.src)
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
            .withFaceLandmarks().withFaceDescriptors()
            // .withFaceExpressions()
            const resizedDetections = faceapi.resizeResults(detections, displaySize)


            const labeledFaceDescriptors = await loadLabeledImages()
            const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, 0.6)

            const results = resizedDetections.map(d => faceMatcher.findBestMatch(d.descriptor))
            results.forEach((result, i) => {
              canvas.getContext('2d').clearRect(0, 0, video.width, video.height)
              const box = resizedDetections[i].detection.box
              if (result._distance <= 5) {
                console.log("success")
              }
              const drawBox = new faceapi.draw.DrawBox(box, { label: result.toString() })
              drawBox.draw(canvas)
            })

          }, 50)
        })

    }

  function startVideo() {
    navigator.getUserMedia(
      { video: {} },
      stream => video.srcObject = stream,
      err => console.error(err)
    )
  }

  function loadLabeledImages() {
    const labels = ['Thang']
    return Promise.all(
      labels.map(async label => {
        const descriptions = []
        for (let i = 1; i <= 2; i++) {

          const avatar = await faceapi.fetchImage(`images/thang1.jpg`)
          const detections = await faceapi.detectSingleFace(avatar)
              .withFaceLandmarks()
              .withFaceDescriptor()
          descriptions.push(detections.descriptor)
          console.log(descriptions)
        }

        return new faceapi.LabeledFaceDescriptors(label, descriptions)
      })
    )
  }
</script> -->
</body>
</html>